---
layout: post
title: "Understanding CopyNet"
excerpt_separator: "<!--more-->"
categories:
  - ai
tags:
  - deep learning
  - neural network
  - neural machine translation
  - question answering
  - copynet
  - seq2seq
  - recurrent neural nets
---

CopyNet [1] is built upon the Seq2Seq framework [2] which improve accuracy by repeated patterns during conversations.
First time hearing the name, I was confused because copying some parts of friend's sentences is not technically interesting.
However, I found that this `copying mechanism` is somewhat realistic: we are often copying each other even what other people are saying.
For example, when your friend tell you that `Hi, my name is Jessy!`, then a popular response would be `Hi, Jessy!`. You should copy the name `Jessy` as a quite normal reaction.
In contrast, if you are too imaginative and instead of `copying`, you `generate` a response like `Hi, Michael` to Jessy, this would be ridiculous.
Then by incorporating the `copying behavior` into RNN Encoder-Decoder framework [2], it would make the question-answering goes smoothly.

<!--more-->

After understanding the motivation of the `copying mechanism`, I have no concern with `copy and paste`.

## Background: Sequence-To-Sequence framework

## CopyNet

## Implementation

### Dataset
I tried the CopyNet for neural machine translation task in the [BilingualCorpus](https://github.com/venali/BilingualCorpus) dataset. This dataset contains 500K pairs of gold Japanese-English sentence pairs (and over 1.3M pairs in total, including non-gold translations).
All the sentences are extracted from 14111 Wikipedia articles.
Sentence pairs are given in 14111 XML document files.
The below code will help to read all XML document files in a directory into a list of bilingual sentence pairs:

```python
import opencorpora
import itertools, os
from glob import glob

def parse_sentence(i):
    if i.tag == 'sen':
        children = i.getchildren()
        jpn_sentences = []
        eng_sentences = []
        for c in children:
            if c.tag == 'j':
                jpn_sentences.append(c.text)
            elif c.tag == 'e':
                eng_sentences.append(c.text)
        return list(itertools.product(jpn_sentences, eng_sentences))
    return []

def parse_document(p):
    pairs = []
    for i in p.getchildren():
        if i.tag == 'sen':
            pairs.extend(parse_sentence(i))
        else:
            pairs.extend(parse_document(i))
    return pairs

def parse_directory(path):
    files = glob(path+"/**/*.xml", recursive=True)
    print("Found {} documents".format(len(files)))
    pairs = []
    for fn in files:
        c = None
        try:
            c = opencorpora.load(fn)
        except Exception as e:
            print(e, fn)
            continue
        if c is not None: 
            pairs.extend(parse_document(c))
    return pairs
```

A sample:
```python
import random
random.choice(pairs)

Out:
('江戸時代には『甲陽軍鑑』が流行し、赤法衣と諏訪法性の兜に象徴される法師武者姿としてのイメージが確立した。',
 'During the Edo period, "Koyo Gunkan" became popular, and the image of Shingen, as Hoshi-musha (armed priests) with a red clerical garment and a helmet of Suwahossho was established.') 
```
### Tokenization
Tokenization is a step-stone in all NLP tasks. For English, I can use space based splitting to do tokenization, but for JP language, when normally all tokens are concatenated without spaces, I use [`spacy`'s JP language tokenizer](https://spacy.io/models/ja#ja_core_news_lg). I don't build tokenizer from scratch but do use a quite powerful and ready-to-use solution, the `spacy`'s one.

```python
from spacy.lang.ja import JapaneseTokenizer
from spacy.lang.en import English
from spacy.tokenizer import Tokenizer
import spacy
nlp = spacy.load('ja_core_news_lg')

tokenizer = JapaneseTokenizer(nlp).tokenizer
eng_tokenizer = Tokenizer(English().vocab)

def contains_digit(string):
    return any(char.isdigit() for char in string)

def create_vocab(pairs):
    vocab = dict()
    eng_vocab = dict()
    for j, e in pairs:
        tokens = list(tokenizer.tokenize(j))
        eng_tokens  list(eng_tokenizer.tokenize(e))
        for token in tokens:
            text = token.normalized_form()
            if not contains_digit(text) and '数' not in token.part_of_speech()[1] \
                and '@' not in text and 'http' not in text and 'www' not in text:
                vocab[text] = vocab.get(text, 0) + 1
        for token in eng_tokens:
            text = token.normalized_form()
            if not contains_digit(text) \
                and '@' not in text and 'http' not in text and 'www' not in text:
                eng_vocab[text] = eng_vocab.get(text, 0) + 1
    return vocab, eng_vocab
```

I found that it is better to remove numerics, phone numbers, emails and URLs from the vocabulary and treat them as OOV. And using bare form (normalized form) of words gave a little boost.
Note for the part `spacy.load('ja_core_news_lg')` that `spacy` also uses a deep learning model for this tokenizer.
## References

{% bibliography --file copynet %}